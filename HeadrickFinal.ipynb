{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-321: Data Mining and Machine Learning\n",
    "\n",
    "# Final Project\n",
    "\n",
    "## Working with scikit-learn\n",
    "## Bobby Headrick\n",
    "\n",
    "### Abstract\n",
    "\n",
    "For this project we run multiple classification algorithms to attempt to classify mushrooms based on whether or not they are edible.  We will be comparing the performance of each algorithm to one another to see which algorithm or algorithms perform best.  We will also compare and analyze the features of our dataset to see which features are best for classifying our data.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "When people see a mushroom in the wilderness, the general consensus is to not eat it.  Many types of mushrooms are edible and appealing to people, but there are also many types which are poisonous.  Unlike poison ivy or othere plants which are harmful to people, there is no one 'rule' for determining if a mushroom is poisonous, such as poison ivy and poison oak's 'leaves of three' rule.  Our goal in this paper to classify mushrooms as either edible or poisonous based on a set of visual features and olfactible features (odor).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "We are attempting to classify mushrooms because it is a problem that can only confidently be solved with a classification success rate of 100%.  Anything lower than that and the classification results have a chance of poisoning someone.  Classification of the mushroom dataset has been done before, such as at https://towardsdatascience.com/building-a-perfect-mushroom-classifier-ceb9d99ae87e.  This project also aimed for perfect classification, again because a classifier for determing whether something is edible or not is only helpful with perfect accuracy.\n",
    "\n",
    "### Data\n",
    "\n",
    "Our dataset contains a variety of mushrooms, described by their physical traits.  The dataset contains 8,124 instances and each instance has 23 features, such as colors of different parts, shapes of different parts, habitat and odor.  For each feature, a letter represents the category name.  We use one-hot encoding to divide the each feature into separate, binary features, with 1 representing that a feature is true for a given mushroom, and 0 representing false.  For example, consider the 'gill spacing' feature.  This has three categories:\n",
    "    1. C = close\n",
    "    2. W = crowded\n",
    "    3. D = distant\n",
    "One-hot encoding divides the gill spacing feature into three features:\n",
    "    1. gill-spacing_c\n",
    "    2. gill-spacing_w\n",
    "    3. gill-spacing_d\n",
    "The reason why we do this instead of changing (C,W,D) to (0,1,2) is because we do not want D to be considered more important than the other two categories just because 2 is the largest of the numbers.  Before one-hot encoding, our dataset has 23 features.  After, there are 118 features.\n",
    "    Our dataset also has some missing values, only in the stalk-root category.  Our one-hot encoding separates these missing values into a separate category, so we remove the column witht the pandas drop method.  Even if this feature is ranked highly in determining whether a mushroom is poisonous, we do not want it because it means nothing to us and can make our classifications incorrect.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "We begin by running a model comparison of classification and regression algorithms on our data and gather the results using the accuracy evaluation metric.  T tests are conducted on the results of the metrics, and models that accept the null hypothesis or perform worse than others are discarded.  \n",
    "    We run a recursive feature elimination ranking function on our data using logistic regression to rank the influence of features on classification.  Only features with a score of 1 are saved.\n",
    "\n",
    "### Results\n",
    "\n",
    "We immediately notice that many models immediately scored 100% success for classifying our data.  The only models that did not acheive this score were ZeroR, Naive Bayes, and a standard SVM.\n",
    "\n",
    "- ZR: 0.517973 (0.017422)\n",
    "- LR: 1.000000 (0.000000)\n",
    "- KN5: 1.000000 (0.000000)\n",
    "- KN7: 1.000000 (0.000000)\n",
    "- DT: 1.000000 (0.000000)\n",
    "- NB: 0.957041 (0.006709)\n",
    "- SVM: 0.999016 (0.001205)\n",
    "- LIN: 1.000000 (0.000000)\n",
    "- RF: 1.000000 (0.000000)\n",
    "- NN: 1.000000 (0.000000)\n",
    "\n",
    "We have a lot of models to choose from, so we run an RFE ranking to evaluate how the algorithms which had 100% success perform when only using features with the highest rankings.  This narrows the feature list down from 117 to 59 using logistic regression as our estimator.  Our results are:\n",
    "\n",
    "- KN5: 1.000000 (0.000000)\n",
    "- KN7: 1.000000 (0.000000)\n",
    "- DT: 1.000000 (0.000000)\n",
    "- LIN: 1.000000 (0.000000)\n",
    "- RF: 1.000000 (0.000000)\n",
    "\n",
    "We plot the decision tree to view which features had the most influence on our classification.\n",
    "<img src=\"files/graph.png\">\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "We see that a mushroom having no odor is the feature with the strongest correlation to whether a mushroom is poisonous or edible.  By splitting our features into binary categories, we are able to accuratly classify our mushrooms, even using only half of the features.  We acheived the necessary 100% success to ensure that our classification is actually useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  117\n",
      "ZR: 0.517973 (0.017422)\n",
      "LR: 1.000000 (0.000000)\n",
      "KN5: 1.000000 (0.000000)\n",
      "KN7: 1.000000 (0.000000)\n",
      "DT: 1.000000 (0.000000)\n",
      "NB: 0.957041 (0.006709)\n",
      "SVM: 0.999016 (0.001205)\n",
      "LIN: 1.000000 (0.000000)\n",
      "RF: 1.000000 (0.000000)\n",
      "NN: 1.000000 (0.000000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Performing t-tests***\n",
      "\n",
      "\n",
      "P-Val between ZeroR and Logistic Regression: 0.00\n",
      "reject null hypothesis\n",
      "\n",
      "P-Val between ZeroR Regression and Decision Tree: 0.00\n",
      "reject null hypothesis\n",
      "\n",
      "\n",
      "***Examining Features***\n",
      "\n",
      "\n",
      "Feature names: ['class' 'cap-shape_b' 'cap-shape_c' 'cap-shape_f' 'cap-shape_k'\n",
      " 'cap-shape_s' 'cap-shape_x' 'cap-surface_f' 'cap-surface_g'\n",
      " 'cap-surface_s' 'cap-surface_y' 'cap-color_b' 'cap-color_c' 'cap-color_e'\n",
      " 'cap-color_g' 'cap-color_n' 'cap-color_p' 'cap-color_r' 'cap-color_u'\n",
      " 'cap-color_w' 'cap-color_y' 'bruises_f' 'bruises_t' 'odor_a' 'odor_c'\n",
      " 'odor_f' 'odor_l' 'odor_m' 'odor_n' 'odor_p' 'odor_s' 'odor_y'\n",
      " 'gill-attachment_a' 'gill-attachment_f' 'gill-spacing_c' 'gill-spacing_w'\n",
      " 'gill-size_b' 'gill-size_n' 'gill-color_b' 'gill-color_e' 'gill-color_g'\n",
      " 'gill-color_h' 'gill-color_k' 'gill-color_n' 'gill-color_o'\n",
      " 'gill-color_p' 'gill-color_r' 'gill-color_u' 'gill-color_w'\n",
      " 'gill-color_y' 'stalk-shape_e' 'stalk-shape_t' 'stalk-root_b'\n",
      " 'stalk-root_c' 'stalk-root_e' 'stalk-root_r' 'stalk-surface-above-ring_f'\n",
      " 'stalk-surface-above-ring_k' 'stalk-surface-above-ring_s'\n",
      " 'stalk-surface-above-ring_y' 'stalk-surface-below-ring_f'\n",
      " 'stalk-surface-below-ring_k' 'stalk-surface-below-ring_s'\n",
      " 'stalk-surface-below-ring_y' 'stalk-color-above-ring_b'\n",
      " 'stalk-color-above-ring_c' 'stalk-color-above-ring_e'\n",
      " 'stalk-color-above-ring_g' 'stalk-color-above-ring_n'\n",
      " 'stalk-color-above-ring_o' 'stalk-color-above-ring_p'\n",
      " 'stalk-color-above-ring_w' 'stalk-color-above-ring_y'\n",
      " 'stalk-color-below-ring_b' 'stalk-color-below-ring_c'\n",
      " 'stalk-color-below-ring_e' 'stalk-color-below-ring_g'\n",
      " 'stalk-color-below-ring_n' 'stalk-color-below-ring_o'\n",
      " 'stalk-color-below-ring_p' 'stalk-color-below-ring_w'\n",
      " 'stalk-color-below-ring_y' 'veil-type_p' 'veil-color_n' 'veil-color_o'\n",
      " 'veil-color_w' 'veil-color_y' 'ring-number_n' 'ring-number_o'\n",
      " 'ring-number_t' 'ring-type_e' 'ring-type_f' 'ring-type_l' 'ring-type_n'\n",
      " 'ring-type_p' 'spore-print-color_b' 'spore-print-color_h'\n",
      " 'spore-print-color_k' 'spore-print-color_n' 'spore-print-color_o'\n",
      " 'spore-print-color_r' 'spore-print-color_u' 'spore-print-color_w'\n",
      " 'spore-print-color_y' 'population_a' 'population_c' 'population_n'\n",
      " 'population_s' 'population_v' 'population_y' 'habitat_d' 'habitat_g'\n",
      " 'habitat_l' 'habitat_m' 'habitat_p' 'habitat_u' 'habitat_w']\n",
      "Feature ranking: [ 4  1 49 55  1 43  1  1 34 52  1  1 53 54 11  1  8  9  1 27 41 26  1  1\n",
      "  1  1 10  1  1  1  1 37 30  1  1  1  1  1  1 59 48 19  2 44 20  1 24 23\n",
      " 29  1  1  1  1 39  1  1  1  1 42  1  6  1  1 28  1  1  5 36  1 16 51  1\n",
      " 38  3  1  7  1  1 21 22  1 50 32 33 31  1 12 56 25  1  1  1 15 57 47  1\n",
      "  1  1 46  1  1 35 45 58  1  1 18 17  1 13  1 40  1 14  1  1]\n",
      "\n",
      "\n",
      "***Slicing data to include ONLY features ranked 1***\n",
      "\n",
      "\n",
      "length:  59\n",
      "LR: 0.999016 (0.001205)\n",
      "KN5: 1.000000 (0.000000)\n",
      "KN7: 1.000000 (0.000000)\n",
      "DT: 1.000000 (0.000000)\n",
      "LIN: 1.000000 (0.000000)\n",
      "RF: 1.000000 (0.000000)\n",
      "NN: 0.990644 (0.004444)\n"
     ]
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "# From scikit learn tutorial\n",
    "# With modification by Nick Webb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Pandas is an important data manipulation library\n",
    "# You don't have to use it for your project, but I include it\n",
    "# here so you know about it.\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "\n",
    "# Import models\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# And import the feature selection mechanism\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# load dataset, using pandas\n",
    "\n",
    "dataframe = pandas.read_csv('mushrooms.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "dataframe['class'] = le.fit_transform(dataframe['class'])\n",
    "\n",
    "dataframe = pandas.get_dummies(dataframe)\n",
    "# drop missing data\n",
    "dataframe = dataframe.drop(columns = ['stalk-root_?'])\n",
    "names = dataframe.columns.values\n",
    "\n",
    "# get data from data frames, as numpy arrays\n",
    "# note that by convention, we use X for input features\n",
    "# and lower case y for the target class\n",
    "\n",
    "array = dataframe.values\n",
    "print('length: ', len(array[0]))\n",
    "X = array[:,1:len(array[0])]\n",
    "y = array[:,0]\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 1\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('ZR', DummyClassifier(strategy=\"most_frequent\")))\n",
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "models.append(('KN5', KNeighborsClassifier()))\n",
    "models.append(('KN7', KNeighborsClassifier(n_neighbors=7)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "models.append(('LIN', SVC(kernel='linear',gamma='auto')))\n",
    "models.append(('RF',RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('NN',MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "# note that I'm going to run through each model above\n",
    "# performing a 10-fold cross-validation each time\n",
    "# (n_splits = 10), specifying 'accuracy' as my measure\n",
    "\n",
    "results = []\n",
    "classifiers = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tclassifiers.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)  \n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(classifiers)\n",
    "plt.show()\n",
    "\n",
    "print('\\n***Performing t-tests***\\n\\n')\n",
    "\n",
    "    \n",
    "ttest,pval = stats.ttest_rel(results[1], results[0])\n",
    "print('P-Val between ZeroR and Logistic Regression: %.2f' % pval)\n",
    "\n",
    "if pval<0.05:\n",
    "    print(\"reject null hypothesis\")\n",
    "else:\n",
    "    print(\"accept null hypothesis\") \n",
    "\n",
    "print()    \n",
    "    \n",
    "ttest,pval = stats.ttest_rel(results[0], results[4])\n",
    "print('P-Val between ZeroR Regression and Decision Tree: %.2f' % pval)\n",
    "\n",
    "if pval<0.05:\n",
    "    print(\"reject null hypothesis\")\n",
    "else:\n",
    "    print(\"accept null hypothesis\")\n",
    "   \n",
    "print('\\n\\n***Examining Features***\\n\\n')\n",
    "\n",
    "log = LogisticRegression(solver='liblinear')\n",
    "rfe = RFE(estimator=log, step=1)\n",
    "rfe.fit(X, y)\n",
    "print(\"Feature names:\",names)\n",
    "print(\"Feature ranking:\",rfe.ranking_)\n",
    "\n",
    "print('\\n\\n***Slicing data to include ONLY features ranked 1***\\n\\n')\n",
    "\n",
    "# I use pandas (badly) to do this, slicing by column names\n",
    "# I extract the column names from the ranking, above\n",
    "# There's better ways to do this, but it's late and I'm tired\n",
    "\n",
    "newCols = []\n",
    "index = 0\n",
    "for i in rfe.ranking_:\n",
    "    if i <= 1:\n",
    "        newCols.append(names[index])\n",
    "    index+=1\n",
    "newCols.append('class')    \n",
    "newData = dataframe[dataframe.columns[dataframe.columns.isin(newCols)]]\n",
    "\n",
    "\n",
    "# Extract the training and test data from the pandas data frame\n",
    "\n",
    "array = newData.values\n",
    "names = newData.columns.values\n",
    "print('length: ', len(array[0]))\n",
    "X = array[:,1:len(array[0])]\n",
    "y = array[:,0]\n",
    "\n",
    "# I'm going to perform a single 10-fold cross-validation\n",
    "# Using my new data, and just two models\n",
    "# Naive Bayes and Logistic Regression\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "models.append(('KN5', KNeighborsClassifier()))\n",
    "models.append(('KN7', KNeighborsClassifier(n_neighbors=7)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('LIN', SVC(kernel='linear',gamma='auto')))\n",
    "models.append(('RF',RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('NN',MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)))\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "    cv_results_4 = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results_4)\n",
    "    classifiers.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results_4.mean(), cv_results_4.std())\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
